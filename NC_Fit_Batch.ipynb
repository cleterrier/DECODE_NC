{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DECODE - Batch Fit SMLM Data\n",
        "The purpose of this notebook is to batch fit acquired sequence using a trained model. Sequence can be tif stacks or nd2 files. For the latter, it uses the [nd2reader](https://github.com/rbnvrw/nd2reader) that can be installed in a DECODE environment using:\n",
        "\n",
        "```pip install nd2reader```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import decode\n",
        "import decode.utils\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "import glob #added for batch\n",
        "from nd2reader import ND2Reader # added for reading nd2\n",
        "\n",
        "print(f\"DECODE version: {decode.utils.bookkeeping.decode_state()}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:31:53.707922Z",
          "start_time": "2020-10-10T16:31:49.338797Z"
        },
        "pycharm": {
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters\n",
        "Set device for inference (i.e. CUDA vs. CPU, for our setup inference on the CPU is about 10 times slower). If you fit on CPU though, you may want to change the number of threads if you have a big machine (see below)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0'  # or 'cpu', or you change cuda device index\n",
        "threads = 12  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
        "worker = 0  # number of workers for data loading. for Windows it only works with 0 at the moment\n",
        "batch = 100 # 32-40 works for 8GB VRAM and 256x256 px frames, 96-100 for the RTX 3090\n",
        "\n",
        "torch.set_num_threads(threads)  # set num threads"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:31:53.716133Z",
          "start_time": "2020-10-10T16:31:53.711877Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set if the fit will use tif sequences or nd2 files as source."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fit_source = 'nd2'\n",
        "#fit_source = 'tif'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify paths for the model, parameters and frames (folder containing source sequences)\n",
        "\n",
        "**Important** If the camera parameters of the training differ from the data which should be fitted (e.g. different EM gain), you can try to use the model anyways, but you must specify them here since we convert to photon units before forwarding through the model."
      ],
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paths to parameters and model\n",
        "\n",
        "#STORM 2D\n",
        "#param_path = 'C:/Users/chris/christo/DECODE/out_NSTORM_STORM647_2D_210415/2021-04-24_10-04-38_NeuroCyto-Proc/param_run.yaml'\n",
        "#model_path = 'C:/Users/chris/christo/DECODE/out_NSTORM_STORM647_2D_210415/2021-04-24_10-04-38_NeuroCyto-Proc/model_0.pt'\n",
        "\n",
        "#STORM 3D\n",
        "param_path = 'C:/Users/Christo/Work/Processing/DECODE10/out_NSTORM_STORM647_3D_210415/2021-04-27_15-54-13_NCIS-Analyse1/param_run.yaml'\n",
        "model_path = 'C:/Users/Christo/Work/Processing/DECODE10/out_NSTORM_STORM647_3D_210415/2021-04-27_15-54-13_NCIS-Analyse1/model_2.pt'\n",
        "\n",
        "# path to folder containing source sequences\n",
        "framefolder_path = 'W:/NC_DATA_NSTORM1_#1/Christo/210430 MAP2#3 (div16) SR/' # don't forget the / at the end\n",
        "\n",
        "# use nd2 files or tif files as source\n",
        "if fit_source == 'nd2':\n",
        "    filelist = glob.glob(framefolder_path + \"*.nd2\")\n",
        "else:\n",
        "    filelist = glob.glob(framefolder_path + \"*.tif\")\n",
        "filelist = glob.glob(framefolder_path + \"*.nd2\")\n",
        "filelist.sort()\n",
        "for filepath in filelist :\n",
        "    filepath = filepath.replace(\"\\\\\", \"/\")\n",
        "    print(filepath)\n",
        "\n",
        "# output path\n",
        "outfolder_path = 'D:/NeuroCyto/data SMLM/210430 MAP2#2 (div16)/Locs ah5/' # don't forget the / at the end\n",
        "    \n",
        "# specify camera parameters of source sequences (if different from model)\n",
        "meta = {\n",
        "    'Camera': { # N-STORM EMCCD parameters\n",
        "        'baseline': 100,\n",
        "        'e_per_adu': 12.48,\n",
        "        'em_gain': 100,\n",
        "        'read_sigma': 74.4,\n",
        "        'spur_noise': 0.002  # if you don't know, you can set this to 0\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:31:53.730382Z",
          "start_time": "2020-10-10T16:31:53.719167Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Parameters and Model\n",
        "Specify Post-Processing as by the parameter file you trained the model with"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "param = decode.utils.param_io.load_params(param_path)\n",
        "model = decode.neuralfitter.models.SigmaMUNet.parse(param)\n",
        "model = decode.utils.model_io.LoadSaveModel(model,\n",
        "                                            input_file=model_path,\n",
        "                                            output_file=None).load_init(device=device)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:31:55.723628Z",
          "start_time": "2020-10-10T16:31:53.743152Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overwrite camera\n",
        "param = decode.utils.param_io.autofill_dict(meta['Camera'], param.to_dict(), mode_missing='include')\n",
        "param = decode.utils.param_io.RecursiveNamespace(**param)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:31:55.723628Z",
          "start_time": "2020-10-10T16:31:53.743152Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit the Data and Export the Resulting Coordinates as .h5 file"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for frame_path in filelist : \n",
        "\n",
        "    # sanitize path\n",
        "    frame_path = frame_path.replace(\"\\\\\", \"/\")\n",
        "   \n",
        "    # import nd2 file\n",
        "    if fit_source == 'nd2':\n",
        "        # load nd2 file    \n",
        "        ndx = ND2Reader(frame_path)\n",
        "        sizes = ndx.sizes\n",
        "\n",
        "        ndx.bundle_axes = 'yx'\n",
        "        ndx.iter_axes = 't'\n",
        "        n = len(ndx)\n",
        "        shape = (sizes['t'], sizes['y'], sizes['x'])\n",
        "        image  = np.zeros(shape, dtype=np.float32)\n",
        "\n",
        "        for i in range(n):\n",
        "            image[i] = ndx.get_frame(i)\n",
        "        image = np.squeeze(image)\n",
        "\n",
        "        frames = torch.from_numpy(image)\n",
        "    # import tif file\n",
        "    else:\n",
        "        frames = decode.utils.frames_io.load_tif(frame_path)\n",
        "        \n",
        "    print(\"processing file \" + frame_path)\n",
        "    print(frames.shape)\n",
        "    \n",
        "    camera = decode.simulation.camera.Photon2Camera.parse(param)\n",
        "    camera.device = 'cpu'\n",
        "    \n",
        "    \n",
        "    # setup frame processing as by the parameter with which the model was trained\n",
        "    frame_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
        "        decode.neuralfitter.utils.processing.wrap_callable(camera.backward),\n",
        "        decode.neuralfitter.frame_processing.AutoCenterCrop(8),\n",
        "        #decode.neuralfitter.frame_processing.Mirror2D(dims=-1),  # WARNING: You might need to comment this line out.\n",
        "        decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
        "    ])\n",
        "\n",
        "    \n",
        "    # determine extent of frame and its dimension after frame_processing\n",
        "    size_procced = decode.neuralfitter.frame_processing.get_frame_extent(frames.unsqueeze(1).size(), frame_proc.forward)  # frame size after processing\n",
        "    frame_extent = ((-0.5, size_procced[-2] - 0.5), (-0.5, size_procced[-1] - 0.5))\n",
        "\n",
        "    \n",
        "    # Setup post-processing\n",
        "    # It's a sequence of backscaling, relative to abs. coord conversion and frame2emitter conversion\n",
        "    post_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
        "\n",
        "        decode.neuralfitter.scale_transform.InverseParamListRescale.parse(param),\n",
        "\n",
        "        decode.neuralfitter.coord_transform.Offset2Coordinate(xextent=frame_extent[0],\n",
        "                                                              yextent=frame_extent[1],\n",
        "                                                              img_shape=size_procced[-2:]),\n",
        "\n",
        "        decode.neuralfitter.post_processing.SpatialIntegration(raw_th=0.1,\n",
        "                                                              xy_unit='px',\n",
        "                                                              px_size=param.Camera.px_size)\n",
        "\n",
        "\n",
        "    ])\n",
        "    \n",
        "    \n",
        "    # fit the data\n",
        "    infer = decode.neuralfitter.Infer(model=model, ch_in=param.HyperParameter.channels_in,\n",
        "                                      frame_proc=frame_proc, post_proc=post_proc,\n",
        "                                      device=device, num_workers=worker, batch_size = batch)\n",
        "\n",
        "    emitter = infer.forward(frames[:])\n",
        "    \n",
        "    \n",
        "    # check on the output\n",
        "    print(emitter)\n",
        "    \n",
        "    \n",
        "    # Check if the predictions look reasonable on a random frame\n",
        "    random_ix = torch.randint(frames.size(0), size=(1, )).item()\n",
        "    em_subset = emitter.get_subset_frame(random_ix, random_ix)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(121)\n",
        "    decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]])).plot()\n",
        "    plt.subplot(122)\n",
        "    decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]]),\n",
        "                               pos_out=em_subset.xyz_px, phot_out=em_subset.prob).plot()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    # Compare the inferred distribution of the photon numbers and background values with the ranges used during training\n",
        "    plt.figure(figsize=(14,4))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    mu, sig = param.Simulation.intensity_mu_sig\n",
        "    plt.axvspan(0, mu+sig*3, color='green', alpha=0.1)\n",
        "    sns.distplot(emitter.phot.numpy())\n",
        "    plt.xlabel('Inferred number of photons')\n",
        "    plt.xlim(0)\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.axvspan(*param.Simulation.bg_uniform, color='green', alpha=0.1)\n",
        "    sns.distplot(emitter.bg.numpy())\n",
        "    plt.xlabel('Inferred background values')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    # plot coordinates histograms\n",
        "    plt.figure(figsize=(18,4))\n",
        "    plt.subplot(131)\n",
        "    plt.hist(emitter.xyz_nm[:, 0].numpy(),100)\n",
        "    plt.xlabel('X (nm)')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.hist(emitter.xyz_nm[:, 1].numpy(),100)\n",
        "    plt.xlabel('Y (nm)')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.hist(emitter.xyz_nm[:, 2].numpy(),100)\n",
        "    plt.xlabel('Z (nm)')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    # plot estimates uncertainties histograms\n",
        "    plt.figure(figsize=(18,4))\n",
        "    plt.subplot(131)\n",
        "    sns.distplot(emitter.xyz_sig_nm[:, 0].numpy())\n",
        "    plt.xlabel('Sigma Estimate in X (nm)')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    sns.distplot(emitter.xyz_sig_nm[:, 1].numpy())\n",
        "    plt.xlabel('Sigma Estimate in Y (nm)')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    sns.distplot(emitter.xyz_sig_nm[:, 2].numpy())\n",
        "    plt.xlabel('Sigma Estimate in Z (nm)')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    # plot raw emitter set\n",
        "    fig, axs = plt.subplots(2,2,figsize=(24, 12), sharex='col', gridspec_kw={'height_ratios':[1,1200/20000]})\n",
        "\n",
        "    decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[-500,500], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_nm[:,2], ax=axs[0,0])\n",
        "    decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], plot_axis=(0,2)).render(emitter, ax=axs[1,0])\n",
        "\n",
        "    decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[0,75], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[0,1])\n",
        "    decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], colextent=[0,75], plot_axis=(0,2)).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[1,1])\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    # h5 save\n",
        "    h5out_path = frame_path.replace(framefolder_path, outfolder_path)\n",
        "    \n",
        "    if fit_source == 'nd2': \n",
        "        h5out_path = h5out_path.replace(\".nd2\", \".h5\")\n",
        "    else:\n",
        "        h5out_path = frame_path.replace(\".tif\", \".h5\") \n",
        "        \n",
        "    emitter.save(h5out_path)  # can be loaded via 'decode.EmitterSet.load('emitter.h5')'\n",
        "    print(\"saved file \" + h5out_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-10T16:39:34.382654Z",
          "start_time": "2020-10-10T16:35:49.690927Z"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}