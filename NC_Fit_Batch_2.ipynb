{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - NeuroCyto Fit Batch 2\n",
    "\n",
    "The purpose of this notebook is to fit all Nikon nd2 sequences within a folder (acquired using the N-STORM microscope). It will identify the channel used tand use the correct trained model for the channel. In the case of a multi-channel acquisition, it will process each channel within the nd2 file with the proper trained model.\n",
    "\n",
    "Currently the two use cases are:\n",
    "- single channel STORM acquisition with 647 nm excitation and 405 nm pump, resulting in a '405/647 R1' channel\n",
    "- two-channel PAINT acquisition with 561 and 647 nm excitation, resulting in two '561 R1' and '647 R1' channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.707922Z",
     "start_time": "2020-10-10T16:31:49.338797Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import decode\n",
    "import decode.utils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import glob #added for batch\n",
    "\n",
    "# Read Nikon nd2 files (nd2reader 3.3.0)\n",
    "# installed using pip in environment, see here: https://rbnvrw.github.io/nd2reader/tutorial.html#installation\n",
    "from nd2reader import ND2Reader # added for reading nd2\n",
    "\n",
    "# Progress bar during file reading\n",
    "# installed ipywidgets and activated them in environment, see \"Installation\", Point 2 here: https://towardsdatascience.com/ever-wanted-progress-bars-in-jupyter-bdb3988d9cfc\n",
    "from tqdm.notebook import tqdm, trange # added for progress bar\n",
    "import time  # added for progress bar\n",
    "\n",
    "# Currently using DECODE 0.10.0 in the decode10_env environment \n",
    "print(f\"DECODE version: {decode.utils.bookkeeping.decode_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters: Acquisition, Inference, Camera parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquisition parameters:** Set the type of acquisition (STORM or PAINT, 2D or 3D) and define the channels identifiers (possible channels = wavelength of each excitation laser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquisition type\n",
    "acqu_type = 'STORM3D' #or STORM2D\n",
    "#acqu_type = 'PAINT3D' #or PAINT2D\n",
    "\n",
    "# Channel identifiers\n",
    "channel_ids = ['488', '561', '647']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference parameters:** Set device for inference (i.e. CUDA vs. CPU, for our setup inference on the CPU is about 10 times slower). If you fit on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.716133Z",
     "start_time": "2020-10-10T16:31:53.711877Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # or 'cpu', or you change cuda device index\n",
    "threads = 8  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 0  # number of workers for data loading. Change only if you know what you are doing.\n",
    "batch = 100 # 32-40 works for 8GB VRAM and 256x256 px, 96 for the RTX 3090\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Camera parameters:** If the camera parameters of the training differ from the data which should be fitted (e.g. different EM gain), you can try to use the model anyways, but you must specify them here since we convert to photon units before forwarding through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.730382Z",
     "start_time": "2020-10-10T16:31:53.719167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify camera parameters of source sequences (if different from model)\n",
    "over_cam = 0\n",
    "meta = {\n",
    "    'Camera': {\n",
    "        'baseline': 100, # N-STORM EMCCD\n",
    "        'e_per_adu': 12.48,\n",
    "        'em_gain': 100,\n",
    "        'read_sigma': 74.4,\n",
    "        'spur_noise': 0.002  # if you don't know, you can set this to 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Specify paths: models, source sequences, and output files paths\n",
    "\n",
    "Here we set up the paths:\n",
    "- root path is where your library of trained models are found\n",
    "- framefolder path is the folder where the source sequences are (folder should contain all nd2 files to be processed and no other nd2 file, as script will try to process all nd2 files it finds within this folder)\n",
    "- outfolder path is the folder where the localization ah5 files will be saved after fitting of each channel of each nd2 file\n",
    "\n",
    "To find the right model, the loops will use two things:\n",
    "- the acquisition type defined at step 2 (STORM3D, PAINT3D)\n",
    "- the wavelength of the excitation laser that is contained in the channel name in the file metadata (488, 561, 647...)\n",
    "\n",
    "So you should store two files: model_1.pt and param_run.yaml from the output folder of your corresponding model training in a 'Current Models' folder (defined as the root path) like this:\n",
    "- Current Models/STORM3D_647/\n",
    "- Current Models/STORM3D_561/\n",
    "- Current Models/STORM3D_647/\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.730382Z",
     "start_time": "2020-10-10T16:31:53.719167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model root path\n",
    "root_path = 'C:/path_to_your_decode_folder/Current models/'\n",
    "\n",
    "# Path to folder containing source sequences\n",
    "framefolder_path = 'W:/path_to_your_nd2_files/' # can be on a server, don't forget the / at the end\n",
    "\n",
    "# Output path for h5 localizations files\n",
    "outfolder_path = 'G:/path_where_you_save_the_ah5_loc_files/'# don't forget the / at the end\n",
    "\n",
    "\n",
    "# Generate the list of nd2 files inside the framefolder\n",
    "filelist = glob.glob(framefolder_path + \"*.nd2\")\n",
    "filelist.sort()\n",
    "for filepath in filelist :\n",
    "    filepath = filepath.replace(\"\\\\\", \"/\")\n",
    "    print(filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part\n",
    "### Loop on nd2 files and channels within, fit the data (with the right model) and save the resulting localisations (h5 file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:34.382654Z",
     "start_time": "2020-10-10T16:35:49.690927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop on nd2 files\n",
    "for frame_path in filelist : \n",
    "\n",
    "    # Sanitize path\n",
    "    frame_path = frame_path.replace(\"\\\\\", \"/\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"processing file \" + frame_path)\n",
    "   \n",
    "    # Load nd2 file    \n",
    "    ndx = ND2Reader(frame_path)\n",
    "    sizes = ndx.sizes\n",
    "    channels = ndx.metadata['channels']\n",
    "    nchan = len(channels)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"   loading nd2 sequence with \" + str(sizes) + \" dimensions with \" + str(nchan) + \" channel(s): \" + str(channels))\n",
    "\n",
    "    ndx.bundle_axes = 'yx'\n",
    "    ndx.iter_axes = 't'\n",
    "    n = len(ndx)   \n",
    "\n",
    "    # Loop on channels\n",
    "    curr_ch = -1;\n",
    "\n",
    "    for channel in channels :\n",
    "\n",
    "        # Define current channel\n",
    "        curr_ch = curr_ch + 1\n",
    "        if nchan > 1:\n",
    "            ndx.default_coords['c'] = curr_ch\n",
    "\n",
    "        # Make image stack to host single-channel frames\n",
    "        shape = (sizes['t'], sizes['y'], sizes['x'])\n",
    "        image  = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        # Assign frames from nd2 file to image stack\n",
    "        for i in tqdm(range(n)):\n",
    "            image[i] = ndx.get_frame(i)\n",
    "\n",
    "        # Convert image stack to torch stack\n",
    "        image = np.squeeze(image)\n",
    "        frames = torch.from_numpy(image)\n",
    "\n",
    "        # Log the import\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"      imported torch stack with \" + str(frames.shape) + \" frames using channel \\'\" + str(channels[curr_ch]) + \"\\'\")\n",
    "\n",
    "        # Find the channel identifier of the current channel\n",
    "        curr_id = '647'\n",
    "        for channel_id in channel_ids:\n",
    "            if channels[curr_ch].find(channel_id) > -1:\n",
    "                curr_id = channel_id\n",
    "\n",
    "        # Build the path to the trained model for the current channel\n",
    "        model_folder = acqu_type + \"_\" + curr_id + \"/\"\n",
    "\n",
    "        # Define path to the model used for fitting\n",
    "        print(\"      using trained model stored in \" + root_path + model_folder)\n",
    "        print(\"\")\n",
    "        param_path = root_path + model_folder + \"param_run.yaml\"\n",
    "        model_path = root_path + model_folder + \"model_1.pt\"\n",
    "\n",
    "        # Load the trained model\n",
    "        param = decode.utils.param_io.load_params(param_path)\n",
    "        model = decode.neuralfitter.models.SigmaMUNet.parse(param)\n",
    "        model = decode.utils.model_io.LoadSaveModel(model,\n",
    "                                        input_file=model_path,\n",
    "                                        output_file=None).load_init(device=device)\n",
    "\n",
    "        # Overwrite camera if necessary\n",
    "        if over_cam > 0:\n",
    "            param = decode.utils.param_io.autofill_dict(meta['Camera'], param.to_dict(), mode_missing='include')\n",
    "            param = decode.utils.param_io.RecursiveNamespace(**param)\n",
    "\n",
    "        # Set camera\n",
    "        camera = decode.simulation.camera.Photon2Camera.parse(param)\n",
    "        camera.device = 'cpu'\n",
    "\n",
    "\n",
    "        # Setup frame processing as by the parameter with which the model was trained\n",
    "        frame_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "            decode.neuralfitter.utils.processing.wrap_callable(camera.backward),\n",
    "            decode.neuralfitter.frame_processing.AutoCenterCrop(8),\n",
    "            #decode.neuralfitter.frame_processing.Mirror2D(dims=-1),  # WARNING: You might need to comment this line out.\n",
    "            decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Determine extent of frame and its dimension after frame_processing\n",
    "        size_procced = decode.neuralfitter.frame_processing.get_frame_extent(frames.unsqueeze(1).size(), frame_proc.forward)  # frame size after processing\n",
    "        frame_extent = ((-0.5, size_procced[-2] - 0.5), (-0.5, size_procced[-1] - 0.5))\n",
    "\n",
    "\n",
    "        # Setup post-processing\n",
    "        # It's a sequence of backscaling, relative to abs. coord conversion and frame2emitter conversion\n",
    "        post_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "\n",
    "            decode.neuralfitter.scale_transform.InverseParamListRescale.parse(param),\n",
    "\n",
    "            decode.neuralfitter.coord_transform.Offset2Coordinate(xextent=frame_extent[0],\n",
    "                                                                  yextent=frame_extent[1],\n",
    "                                                                  img_shape=size_procced[-2:]),\n",
    "\n",
    "            decode.neuralfitter.post_processing.SpatialIntegration(raw_th=0.1,\n",
    "                                                                  xy_unit='px',\n",
    "                                                                  px_size=param.Camera.px_size)\n",
    "\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Fit the data\n",
    "        infer = decode.neuralfitter.Infer(model=model, ch_in=param.HyperParameter.channels_in,\n",
    "                                          frame_proc=frame_proc, post_proc=post_proc,\n",
    "                                          device=device, num_workers=worker, batch_size = batch)\n",
    "\n",
    "        emitter = infer.forward(frames[:])\n",
    "\n",
    "\n",
    "        # Check on the output\n",
    "        print(emitter)\n",
    "\n",
    "\n",
    "        # Check if the predictions look reasonable on a random frame\n",
    "        random_ix = torch.randint(frames.size(0), size=(1, )).item()\n",
    "        em_subset = emitter.get_subset_frame(random_ix, random_ix)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(121)\n",
    "        decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]])).plot()\n",
    "        plt.subplot(122)\n",
    "        decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]]),\n",
    "                                   pos_out=em_subset.xyz_px, phot_out=em_subset.prob).plot()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Compare the inferred distribution of the photon numbers and background values with the ranges used during training\n",
    "        plt.figure(figsize=(14,4))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        mu, sig = param.Simulation.intensity_mu_sig\n",
    "        plt.axvspan(0, mu+sig*3, color='green', alpha=0.1)\n",
    "        sns.distplot(emitter.phot.numpy())\n",
    "        plt.xlabel('Inferred number of photons')\n",
    "        plt.xlim(0, 10000)\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.axvspan(*param.Simulation.bg_uniform, color='green', alpha=0.1)\n",
    "        sns.distplot(emitter.bg.numpy())\n",
    "        plt.xlabel('Inferred background values')\n",
    "        plt.xlim(0, 400)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot coordinates histograms\n",
    "        plt.figure(figsize=(18,4))\n",
    "        plt.subplot(131)\n",
    "        plt.hist(emitter.xyz_nm[:, 0].numpy(),100)\n",
    "        plt.xlabel('X (nm)')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.hist(emitter.xyz_nm[:, 1].numpy(),100)\n",
    "        plt.xlabel('Y (nm)')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.hist(emitter.xyz_nm[:, 2].numpy(),100)\n",
    "        plt.xlabel('Z (nm)')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot uncertainties histograms\n",
    "        plt.figure(figsize=(18,4))\n",
    "        plt.subplot(131)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 0].numpy())\n",
    "        plt.xlabel('Sigma Estimate in X (nm)')\n",
    "        plt.xlim(0, 100)\n",
    "\n",
    "        plt.subplot(132)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 1].numpy())\n",
    "        plt.xlabel('Sigma Estimate in Y (nm)')\n",
    "        plt.xlim(0, 100)\n",
    "\n",
    "        plt.subplot(133)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 2].numpy())\n",
    "        plt.xlabel('Sigma Estimate in Z (nm)')\n",
    "        plt.xlim(0, 300)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot raw emitter set\n",
    "        fig, axs = plt.subplots(2,2,figsize=(24, 12), sharex='col', gridspec_kw={'height_ratios':[1,1200/20000]})\n",
    "\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[-500,500], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_nm[:,2], ax=axs[0,0])\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], plot_axis=(0,2)).render(emitter, ax=axs[1,0])\n",
    "\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[0,75], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[0,1])\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], colextent=[0,75], plot_axis=(0,2)).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[1,1])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # h5 save\n",
    "        h5out_path = frame_path.replace(framefolder_path, outfolder_path)\n",
    "        h5out_path = h5out_path.replace(\".nd2\", \"_\" + curr_id + \".h5\")\n",
    "\n",
    "        emitter.save(h5out_path)  # can be loaded via 'decode.EmitterSet.load('emitter.h5')'\n",
    "        print(\"saved file \" + h5out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
