{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - NeuroCyto Fit Batch 2\n",
    "\n",
    "The purpose of this notebook is to fit all Nikon nd2 sequences within a folder (acquired using the N-STORM microscope). It will identify the channel used tand use the correct trained model for the channel. In the case of a multi-channel acquisition, it will process each channel within the nd2 file with the proper trained model.\n",
    "\n",
    "Currently the two use cases are:\n",
    "- single channel STORM acquisition with 647 nm excitation and 405 nm pump, resulting in a '405/647 R1' channel\n",
    "- two-channel PAINT acquisition with 561 and 647 nm excitation, resulting in two '561 R1' and '647 R1' channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.707922Z",
     "start_time": "2020-10-10T16:31:49.338797Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECODE version: 0.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import decode\n",
    "import decode.utils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import glob #added for batch\n",
    "from nd2reader import ND2Reader # added for reading nd2\n",
    "\n",
    "# Progress bar during file reading\n",
    "# installed ipywidgets and activated them in environment, see \"Installation\", Point 2 here: https://towardsdatascience.com/ever-wanted-progress-bars-in-jupyter-bdb3988d9cfc\n",
    "from tqdm.notebook import tqdm, trange # added for progress bar\n",
    "import time  # added for progress bar\n",
    "\n",
    "print(f\"DECODE version: {decode.utils.bookkeeping.decode_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters: Acquisition, Inference, Camera parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquisition parameters:** Set the type of acquisition (STORM or PAINT, 2D or 3D) and define the channels identifiers (possible channels = wavelength of each excitation laser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquisition type\n",
    "acqu_type = 'STORM3D' #or STORM2D\n",
    "#acqu_type = 'PAINT3D' #or PAINT2D\n",
    "\n",
    "# Channel identifiers\n",
    "channel_ids = ['488', '561', '647']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference parameters:** Set device for inference (i.e. CUDA vs. CPU, for our setup inference on the CPU is about 10 times slower). If you fit on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.716133Z",
     "start_time": "2020-10-10T16:31:53.711877Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # or 'cpu', or you change cuda device index\n",
    "threads = 8  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 0  # number of workers for data loading. Change only if you know what you are doing.\n",
    "batch = 100 # 32-40 works for 8GB VRAM and 256x256 px, 96 for the RTX 3090\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Camera parameters:** If the camera parameters of the training differ from the data which should be fitted (e.g. different EM gain), you can try to use the model anyways, but you must specify them here since we convert to photon units before forwarding through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.730382Z",
     "start_time": "2020-10-10T16:31:53.719167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify camera parameters of source sequences (if different from model)\n",
    "over_cam = 0\n",
    "meta = {\n",
    "    'Camera': {\n",
    "        'baseline': 100, # N-STORM EMCCD\n",
    "        'e_per_adu': 12.48,\n",
    "        'em_gain': 100,\n",
    "        'read_sigma': 74.4,\n",
    "        'spur_noise': 0.002  # if you don't know, you can set this to 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Specify paths: models, source sequences, and output files paths\n",
    "\n",
    "Here we set up the paths:\n",
    "- root path is where your library of trained models are found\n",
    "- framefolder path is the folder where the source sequences are (folder should contain all nd2 files to be processed and no other nd2 file, as script will try to process all nd2 files it finds within this folder)\n",
    "- outfolder path is the folder where the localization ah5 files will be saved after fitting of each channel of each nd2 file\n",
    "\n",
    "To find the right model, the loops will use two things:\n",
    "- the acquisition type defined at step 2 (STORM3D, PAINT3D)\n",
    "- the wavelength of the excitation laser that is contained in the channel name in the file metadata (488, 561, 647...)\n",
    "\n",
    "So you should store two files: model_1.pt and param_run.yaml from the output folder of your corresponding model training in a ' folder called like this:\n",
    "- Current Models/STORM3D_647\n",
    "- Current Models/STORM3D_561\n",
    "- Current Models/STORM3D_647\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.730382Z",
     "start_time": "2020-10-10T16:31:53.719167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N1a_div7_ph-add-b2s-m2.nd2\n",
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N1b_div7_ph-add-b2s-m2.nd2\n",
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N1c_div7_ph-add-b2s-m2.nd2\n",
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N2a_div7_ph-add-b2s-m2.nd2\n",
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N2b_div7_ph-add-b2s-m2.nd2\n",
      "W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N2c_div7_ph-add-b2s-m2.nd2\n"
     ]
    }
   ],
   "source": [
    "# Model root path\n",
    "root_path = 'C:/Users/chris/christo/Processing/DECODE10/Current models/'\n",
    "\n",
    "# Path to folder containing source sequences\n",
    "framefolder_path = 'W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/' # don't forget the / at the end\n",
    "# framefolder_path = 'C:/Users/chris/Desktop/test/' # don't forget the / at the end\n",
    "\n",
    "# Output path for h5 localizations files\n",
    "outfolder_path = 'G:/data SMLM/210518 MPS#7 (div7 actin) DC2/Locs ah5/'# don't forget the / at the end\n",
    "\n",
    "\n",
    "# Generate the list of nd2 files inside the framefolder\n",
    "filelist = glob.glob(framefolder_path + \"*.nd2\")\n",
    "filelist.sort()\n",
    "for filepath in filelist :\n",
    "    filepath = filepath.replace(\"\\\\\", \"/\")\n",
    "    print(filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part\n",
    "### Loop on nd2 files and channels within, fit the data (with the right model) and save the resulting localisations (h5 file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:34.382654Z",
     "start_time": "2020-10-10T16:35:49.690927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "processing file W:/NC_DATA_NSTORM1_#1/Christo/210518 MPS#7 (div7 actin) SR/C1_N1a_div7_ph-add-b2s-m2.nd2\n",
      "\n",
      "   loaded nd2 sequence with {'x': 256, 'y': 256, 't': 60000} dimensions with 1 channel(s): ['405/647 R1']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb73163d2742e0a82bcb4c5215685f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-2b26520bbec0>\", line 39, in <module>\n",
      "    image[i] = ndx.get_frame(i)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\pims\\base_frames.py\", line 592, in get_frame\n",
      "    result = self._get_frame_wrapped(**coords)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\reader.py\", line 88, in get_frame_2D\n",
      "    return self._parser.get_image_by_attributes(t, v, c, z, y, x)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\parser.py\", line 103, in get_image_by_attributes\n",
      "    timestamp, raw_image_data = self._get_raw_image_data(image_group_number, channel,\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\parser.py\", line 262, in _get_raw_image_data\n",
      "    data = read_chunk(self._fh, chunk)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\common.py\", line 69, in read_chunk\n",
      "    return fh.read(data_length)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\chris\\.conda\\envs\\decode10_env\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b26520bbec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\pims\\base_frames.py\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_frame_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\reader.py\u001b[0m in \u001b[0;36mget_frame_2D\u001b[1;34m(self, c, t, z, x, y, v)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_image_by_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\parser.py\u001b[0m in \u001b[0;36mget_image_by_attributes\u001b[1;34m(self, frame_number, field_of_view, channel, z_level, height, width)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             timestamp, raw_image_data = self._get_raw_image_data(image_group_number, channel,\n\u001b[0m\u001b[0;32m    104\u001b[0m                                                                  height, width)\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\parser.py\u001b[0m in \u001b[0;36m_get_raw_image_data\u001b[1;34m(self, image_group_number, channel_offset, height, width)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_image_data_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_group_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\nd2reader\\common.py\u001b[0m in \u001b[0;36mread_chunk\u001b[1;34m(fh, chunk_location)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_location\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m16\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decode10_env\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Loop on nd2 files\n",
    "for frame_path in filelist : \n",
    "\n",
    "    # Sanitize path\n",
    "    frame_path = frame_path.replace(\"\\\\\", \"/\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"processing file \" + frame_path)\n",
    "   \n",
    "    # Load nd2 file    \n",
    "    ndx = ND2Reader(frame_path)\n",
    "    sizes = ndx.sizes\n",
    "    channels = ndx.metadata['channels']\n",
    "    nchan = len(channels)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"   loading nd2 sequence with \" + str(sizes) + \" dimensions with \" + str(nchan) + \" channel(s): \" + str(channels))\n",
    "\n",
    "    ndx.bundle_axes = 'yx'\n",
    "    ndx.iter_axes = 't'\n",
    "    n = len(ndx)   \n",
    "\n",
    "    # Loop on channels\n",
    "    curr_ch = -1;\n",
    "\n",
    "    for channel in channels :\n",
    "\n",
    "        # Define current channel\n",
    "        curr_ch = curr_ch + 1\n",
    "        if nchan > 1:\n",
    "            ndx.default_coords['c'] = curr_ch\n",
    "\n",
    "        # Make image stack to host single-channel frames\n",
    "        shape = (sizes['t'], sizes['y'], sizes['x'])\n",
    "        image  = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        # Assign frames from nd2 file to image stack\n",
    "        for i in tqdm(range(n)):\n",
    "            image[i] = ndx.get_frame(i)\n",
    "\n",
    "        # Convert image stack to torch stack\n",
    "        image = np.squeeze(image)\n",
    "        frames = torch.from_numpy(image)\n",
    "\n",
    "        # Log the import\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"      imported torch stack with \" + str(frames.shape) + \" frames using channel \\'\" + str(channels[curr_ch]) + \"\\'\")\n",
    "\n",
    "        # Find the channel identifier of the current channel\n",
    "        curr_id = '647'\n",
    "        for channel_id in channel_ids:\n",
    "            if channels[curr_ch].find(channel_id) > -1:\n",
    "                curr_id = channel_id\n",
    "\n",
    "        # Build the path to the trained model for the current channel\n",
    "        model_folder = acqu_type + \"_\" + curr_id + \"/\"\n",
    "\n",
    "        # Define path to the model used for fitting\n",
    "        print(\"      using trained model stored in \" + root_path + model_folder)\n",
    "        print(\"\")\n",
    "        param_path = root_path + model_folder + \"param_run.yaml\"\n",
    "        model_path = root_path + model_folder + \"model_1.pt\"\n",
    "\n",
    "        # Load the trained model\n",
    "        param = decode.utils.param_io.load_params(param_path)\n",
    "        model = decode.neuralfitter.models.SigmaMUNet.parse(param)\n",
    "        model = decode.utils.model_io.LoadSaveModel(model,\n",
    "                                        input_file=model_path,\n",
    "                                        output_file=None).load_init(device=device)\n",
    "\n",
    "        # Overwrite camera if necessary\n",
    "        if over_cam > 0:\n",
    "            param = decode.utils.param_io.autofill_dict(meta['Camera'], param.to_dict(), mode_missing='include')\n",
    "            param = decode.utils.param_io.RecursiveNamespace(**param)\n",
    "\n",
    "        # Set camera\n",
    "        camera = decode.simulation.camera.Photon2Camera.parse(param)\n",
    "        camera.device = 'cpu'\n",
    "\n",
    "\n",
    "        # Setup frame processing as by the parameter with which the model was trained\n",
    "        frame_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "            decode.neuralfitter.utils.processing.wrap_callable(camera.backward),\n",
    "            decode.neuralfitter.frame_processing.AutoCenterCrop(8),\n",
    "            #decode.neuralfitter.frame_processing.Mirror2D(dims=-1),  # WARNING: You might need to comment this line out.\n",
    "            decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Determine extent of frame and its dimension after frame_processing\n",
    "        size_procced = decode.neuralfitter.frame_processing.get_frame_extent(frames.unsqueeze(1).size(), frame_proc.forward)  # frame size after processing\n",
    "        frame_extent = ((-0.5, size_procced[-2] - 0.5), (-0.5, size_procced[-1] - 0.5))\n",
    "\n",
    "\n",
    "        # Setup post-processing\n",
    "        # It's a sequence of backscaling, relative to abs. coord conversion and frame2emitter conversion\n",
    "        post_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "\n",
    "            decode.neuralfitter.scale_transform.InverseParamListRescale.parse(param),\n",
    "\n",
    "            decode.neuralfitter.coord_transform.Offset2Coordinate(xextent=frame_extent[0],\n",
    "                                                                  yextent=frame_extent[1],\n",
    "                                                                  img_shape=size_procced[-2:]),\n",
    "\n",
    "            decode.neuralfitter.post_processing.SpatialIntegration(raw_th=0.1,\n",
    "                                                                  xy_unit='px',\n",
    "                                                                  px_size=param.Camera.px_size)\n",
    "\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Fit the data\n",
    "        infer = decode.neuralfitter.Infer(model=model, ch_in=param.HyperParameter.channels_in,\n",
    "                                          frame_proc=frame_proc, post_proc=post_proc,\n",
    "                                          device=device, num_workers=worker, batch_size = batch)\n",
    "\n",
    "        emitter = infer.forward(frames[:])\n",
    "\n",
    "\n",
    "        # Check on the output\n",
    "        print(emitter)\n",
    "\n",
    "\n",
    "        # Check if the predictions look reasonable on a random frame\n",
    "        random_ix = torch.randint(frames.size(0), size=(1, )).item()\n",
    "        em_subset = emitter.get_subset_frame(random_ix, random_ix)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(121)\n",
    "        decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]])).plot()\n",
    "        plt.subplot(122)\n",
    "        decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]]),\n",
    "                                   pos_out=em_subset.xyz_px, phot_out=em_subset.prob).plot()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Compare the inferred distribution of the photon numbers and background values with the ranges used during training\n",
    "        plt.figure(figsize=(14,4))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        mu, sig = param.Simulation.intensity_mu_sig\n",
    "        plt.axvspan(0, mu+sig*3, color='green', alpha=0.1)\n",
    "        sns.distplot(emitter.phot.numpy())\n",
    "        plt.xlabel('Inferred number of photons')\n",
    "        plt.xlim(0, 10000)\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.axvspan(*param.Simulation.bg_uniform, color='green', alpha=0.1)\n",
    "        sns.distplot(emitter.bg.numpy())\n",
    "        plt.xlabel('Inferred background values')\n",
    "        plt.xlim(0, 400)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot coordinates histograms\n",
    "        plt.figure(figsize=(18,4))\n",
    "        plt.subplot(131)\n",
    "        plt.hist(emitter.xyz_nm[:, 0].numpy(),100)\n",
    "        plt.xlabel('X (nm)')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.hist(emitter.xyz_nm[:, 1].numpy(),100)\n",
    "        plt.xlabel('Y (nm)')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.hist(emitter.xyz_nm[:, 2].numpy(),100)\n",
    "        plt.xlabel('Z (nm)')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot uncertainties histograms\n",
    "        plt.figure(figsize=(18,4))\n",
    "        plt.subplot(131)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 0].numpy())\n",
    "        plt.xlabel('Sigma Estimate in X (nm)')\n",
    "        plt.xlim(0, 100)\n",
    "\n",
    "        plt.subplot(132)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 1].numpy())\n",
    "        plt.xlabel('Sigma Estimate in Y (nm)')\n",
    "        plt.xlim(0, 100)\n",
    "\n",
    "        plt.subplot(133)\n",
    "        sns.distplot(emitter.xyz_sig_nm[:, 2].numpy())\n",
    "        plt.xlabel('Sigma Estimate in Z (nm)')\n",
    "        plt.xlim(0, 300)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # plot raw emitter set\n",
    "        fig, axs = plt.subplots(2,2,figsize=(24, 12), sharex='col', gridspec_kw={'height_ratios':[1,1200/20000]})\n",
    "\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[-500,500], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_nm[:,2], ax=axs[0,0])\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], plot_axis=(0,2)).render(emitter, ax=axs[1,0])\n",
    "\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5, zextent=[-600,600], colextent=[0,75], plot_axis=(0,1), contrast=1.25).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[0,1])\n",
    "        decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50, zextent=[-600,600], colextent=[0,75], plot_axis=(0,2)).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[1,1])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # h5 save\n",
    "        h5out_path = frame_path.replace(framefolder_path, outfolder_path)\n",
    "        h5out_path = h5out_path.replace(\".nd2\", \"_\" + curr_id + \".h5\")\n",
    "\n",
    "        emitter.save(h5out_path)  # can be loaded via 'decode.EmitterSet.load('emitter.h5')'\n",
    "        print(\"saved file \" + h5out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
